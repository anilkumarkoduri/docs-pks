---
title: Enterprise PKS Release Notes
owner: PKS
topictype: releasenotes
---

<strong><%= modified_date %></strong>

This topic contains release notes for <%= vars.product_full %> v1.4.x.

## <a id="1.4.3"></a>v1.4.3

**Release Date**: October 11, 2019

### <a id="v1.4.3-features"></a>Features

* **[Feature Improvement]** Adds resource types, such as `StatefulSet` and `DaemonSet`, and container image names to data captured by PKS Telemetry.
* **[Security Fix]** Bumps Kubernetes to v1.13.10.
* **[Security Fix]** Bumps Docker to v18.09.8.
* **[Security Fix]** Bumps UAA to v71.3.
* **[Security Fix]** Bumps Ruby to v2.6.3 for <%= vars.product_short %> Telemetry.
* **[Security Fix]** Bumps Golang to v1.12.9 for <%= vars.product_short %> NSX-T Networking and Telemetry.
* **[Security Fix]** Updates container images for sink resources to address Golang CVE-2019-9512 and CVE-2019-9514.
* **[Bug Fix]** Adds the `hostPath` volume to the PKS Privileged Pod Security Policy.
* **[Bug Fix]** Resolves an issue with group role bindings that affects the `pks get-credentials` and `pks get-kubeconfig` commands.
Previously, if a `RoleBinding` was defined for a `Group` instead of an individual `User`,
users of that group were not able to access their Kubernetes cluster with the permissions defined in the role.
* **[Bug Fix]** Resolves an issue where kubectl reports an error when trying to refresh an expired ID token.
* **[Bug Fix]** Resolves an issue where syslog messages transmitted over TCP
are truncated to 1,024 bytes before they reach the syslog endpoint configured
in the **<%= vars.product_tile %>** tile -> **Logging** -> **Enable Syslog for PKS**.
* **[Bug Fix]** Resolves an issue where Observability Manager deletes the `pks-system` namespace and all pods in the namespace during cluster creation if
an operator disables both log and metric sink resources in the <%= vars.product_tile %> tile.
* **[Bug Fix]** Resolves an issue where logs sent to log sinks have an `_` character in the hostname.

### <a id="v1.4.3-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.4.3</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>October 11, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.4.2+, v2.5.x, v2.6.x</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions (Azure only)</td>
        <td>v2.4.2, v2.4.3, v2.4.13, v2.5.5 and later, v2.6.x</td>
    </tr>
    <tr>
        <td>Xenial stemcell version</td>
        <td>v250.93</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.13.10</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.26.0</td>
    </tr>
    <tr>
        <td>NSX-T versions</td>
        <td>v2.3.1, v2.4.0.1, v2.4.1, v2.4.2 (<a href="#2-4-2">see below</a>)</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.4.1</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.09.8</td>
    </tr>
    <tr>
        <td>Backup and Restore SDK version</td>
        <td>v1.8.0</td>
    </tr>
    <tr>
        <td>UAA version</td>
        <td>v71.3</td>
    </tr>
</table>

<p class="note warning"><strong>Warning:</strong> NSX-T v2.4 implements a new password expiration policy.
By default, the administrator password expires after 90 days. If the password expires,
you cannot deploy new Kubernetes clusters using PKS with NSX-T. For more information, see the
following <a href="https://kb.vmware.com/s/article/70691">VMware KB article</a>
and <a href="./nsxt-deploy-24.html#nsxt-24-password">Updating the NSX Manager Password for NSX-T v2.4</a> in the
<%= vars.product_short %> documentation.</p>

<p class="note"><strong>Note:</strong> NSX-T v2.4 implements a new user interface (UI) based on the NSX Policy API.
PKS v1.4.3 does not support the NSX Policy API. Any objects created through the new UI cannot be used with PKS v1.4.3.
If you are installing PKS v1.4.3 with NSX-T v2.4.x or upgrading to PKS v1.4.3 and NSX-T 2.4.x,
you must use the <strong>Advanced Networking</strong> tab in NSX Manager to create, read, update, and delete
all networking objects required for <%= vars.product_short %>. For more information, see <a href="./nsxt-deploy-24.html#ui-nsxt-24">NSX-T v2.4 Management Interfaces</a> in the
<%= vars.product_short %> documentation.</p>

### <a id='vsphere-reqs'></a> vSphere Version Requirements

If you are installing <%= vars.product_short %> on vSphere or on vSphere with NSX-T Data Center,
see the <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a> for compatibility information.</p>

### <a id="v1.4.3-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</sup></td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
</table>

<sup>&#42;</sup> For more information about configuring Service `type:LoadBalancer` on AWS, see the [Access Workloads Using an Internal AWS Load Balancer](deploy-workloads.html#internal-lb) section of _Deploying and Exposing Basic Workloads_.

### <a id="v1.4.3-upgrade"></a>Upgrade Path

The supported upgrade paths to <%= vars.product_short %> v1.4.3 are from <%= vars.product_short %> v1.4.0 and v1.3.4 and later.

<p class="note warning"><strong>WARNING:</strong> Before you upgrade to <%= vars.product_short %> 1.4.x, you must increase the size of persistent disk for the PKS VM. 
For more information,
see <a href="#not-enough-diskspace">Upgrade Fails Due to Insufficient Disk Space</a> in the Known Issues.</p>

To upgrade, see [Upgrading <%= vars.product_short %>](upgrade-pks.html) and [Upgrading <%= vars.product_short %> with NSX-T](upgrade-pks-nsxt.html).

<p class="note breaking"><strong>Breaking change:</strong> Upgrading to Ops Manager v2.6 on vSphere requires that you set a public SSH key in the OVF template prior to the upgrade. For more information, see <a href="#no-password-om">Passwords Not Supported for Ops Manager VM on vSphere</a> in the Breaking Changes section.</p>

### <a id="v1.4.3-breaking-changes"></a> Breaking Changes

For information about breaking changes in <%= vars.product_short %> v1.4.3,
see the following:

* [Breaking Changes in <%= vars.product_short %> 1.4.0](#v1.4.0-breaking-changes)
* [Breaking Changes in <%= vars.product_short %> 1.4.1](#v1.4.1-breaking-changes)

### <a id="v1.4.3-known-issues"></a> Known Issues

<%= vars.product_short %> v1.4.3 has the following known issues:

#### <a id='2-4-2'></a> Support for NSX-T v2.4.2 with Known Issue and Workaround

<%= vars.product_short %> v1.4.3 supports NSX-T v2.4.2. However, there is a known issue with NSX-T v2.4.2 that can affect new and upgraded
installations of <%= vars.product_short %> v1.4.3 that use a [NAT topology](./nsxt-topologies.html#topology-nat).

For NSX-T v2.4.2, the PKS Management Plane must be deployed on a Tier-1 distributed router (DR). If the PKS Management Plane is deployed on a Tier-1 service router (SR), the router needs to be converted. To convert an SR to a DR, refer to the [East-West traffic between workloads behind different T1 is impacted, when NAT is configured on T0 (71363)](https://kb.vmware.com/s/article/71363) KB article.

This issue is addressed in NSX-T v2.5 so that it does not matter if the Tier-1 Router is a DR or an SR.

#### <a id='other'></a> Other Known Issues in <%= vars.product_short %> v1.4.3

In addition to the known issue listed above, see the following:

* [Known Issues in <%= vars.product_short %> 1.4.1](#v1.4.1-known-issues)
* [Known Issues in <%= vars.product_short %> 1.4.0](#v1.4.0-known-issues)

## <a id="1.4.2"></a>v1.4.2

**Release Date**: August 19, 2019

### <a id="v1.4.2-features"></a>Features

* **[Security Fix]** This patch addresses [CVE-2019-3794](https://nvd.nist.gov/vuln/detail/CVE-2019-3794), the UAA `client.write` scope vulnerability.
* **[Bug Fix]** Updated NCP timeout parameters to ensure that NCP does not overwhelm NSX-T in a high scale environment.

### <a id="v1.4.2-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.4.2</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>August 19, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.4.2+, v2.5.x, v2.6.x</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions (Azure only)</td>
        <td>v2.4.2, v2.4.3, v2.4.13, v2.5.5 and later, v2.6.x</td>
    </tr>
    <tr>
        <td>Xenial stemcell version</td>
        <td>v250.73</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.13.5</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.26.0</td>
    </tr>
    <tr>
        <td>NSX-T versions</td>
        <td>v2.3.1, v2.4.0.1, v2.4.1, v2.4.2 (<a href="#nsxt-242">see below</a>)</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.4.1</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.3-ce</td>
    </tr>
    <tr>
        <td>Backup and Restore SDK version</td>
        <td>v1.8.0</td>
    </tr>
    <tr>
        <td>UAA version</td>
        <td>v71.2</td>
    </tr>
</table>

<p class="note warning"><strong>Warning:</strong> NSX-T v2.4 implements a new password expiration policy.
By default, the administrator password expires after 90 days. If the password expires,
you cannot deploy new Kubernetes clusters using PKS with NSX-T. For more information, see the
following <a href="https://kb.vmware.com/s/article/70691">VMware KB article</a>
and <a href="./nsxt-deploy-24.html#nsxt-24-password">Updating the NSX Manager Password for NSX-T v2.4</a> in the
<%= vars.product_short %> documentation.</p>

<p class="note"><strong>Note:</strong> NSX-T v2.4 implements a new user interface (UI) based on the NSX Policy API.
PKS v1.4.2 does not support the NSX Policy API. Any objects created through the new UI cannot be used with PKS v1.4.2.
If you are installing PKS v1.4.2 with NSX-T v2.4.x or upgrading to PKS v1.4.2 and NSX-T 2.4.x,
you must use the <strong>Advanced Networking</strong> tab in NSX Manager to create, read, update, and delete
all networking objects required for <%= vars.product_short %>. For more information, see <a href="./nsxt-deploy-24.html#ui-nsxt-24">NSX-T v2.4 Management Interfaces</a> in the
<%= vars.product_short %> documentation.</p>

### <a id='vsphere-reqs'></a> vSphere Version Requirements

If you are installing <%= vars.product_short %> on vSphere or on vSphere with NSX-T Data Center,
see the <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a> for compatibility information.</p>

### <a id="v1.4.2-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</sup></td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
</table>

<sup>&#42;</sup> For more information about configuring Service `type:LoadBalancer` on AWS, see the [Access Workloads Using an Internal AWS Load Balancer](deploy-workloads.html#internal-lb) section of _Deploying and Exposing Basic Workloads_.

### <a id="v1.4.2-upgrade"></a>Upgrade Path

The supported upgrade paths to PKS v1.4.2 are from PKS v1.4.0 and PKS v1.3.4 and later.

<p class="note warning"><strong>WARNING:</strong> Before you upgrade to PKS 1.4.x, you must increase the size of persistent disk for the PKS VM. 
    For more information, 
    see <a href="#not-enough-diskspace">Upgrade Fails Due to Insufficient Disk Space</a> in the Known Issues.</p>

To upgrade, see [Upgrading <%= vars.product_short %>](upgrade-pks.html) and [Upgrading <%= vars.product_short %> with NSX-T](upgrade-pks-nsxt.html).

<p class="note breaking"><strong>Breaking change:</strong> Upgrading to Ops Manager v2.6 on vSphere requires that you set a public SSH key in the OVF template prior to the upgrade. For more information, see <a href="#no-password-om">Passwords Not Supported for Ops Manager VM on vSphere</a> in the Breaking Changes section.</p>

### <a id="v1.4.2-breaking-changes"></a> Breaking Changes

For information about breaking changes in <%= vars.product_short %> v1.4.2,
see the following:

* [Breaking Changes in <%= vars.product_short %> 1.4.1](#v1.4.1-breaking-changes)
* [Breaking Changes in <%= vars.product_short %> 1.4.0](#v1.4.0-breaking-changes)

### <a id="v1.4.2-known-issues"></a> Known Issues

<%= vars.product_short %> v1.4.2 has the following known issues:

#### <a id='nsxt-242'></a> Support for NSX-T v2.4.2 with Known Issue and Workaround

<%= vars.product_short %> v1.4.2 supports NSX-T v2.4.2. However, there is a known issue with NSX-T v2.4.2 that can affect new and upgraded
installations of <%= vars.product_short %> v1.4.2 that use a [NAT topology](./nsxt-topologies.html#topology-nat).

For NSX-T v2.4.2, the PKS Management Plane must be deployed on a Tier-1 distributed router (DR). If the PKS Management Plane is deployed on a Tier-1 service router (SR), the router needs to be converted. To convert an SR to a DR, refer to the [East-West traffic between workloads behind different T1 is impacted, when NAT is configured on T0 (71363)](https://kb.vmware.com/s/article/71363) KB article.

This issue will be addressed in a subsequent release of NSX-T such that it will not matter if the Tier-1 Router is a DR or an SR.

#### <a id='other'></a> Other Known Issues in <%= vars.product_short %> v1.4.2

In addition to the known issue listed above, see the following:

* [Known Issues in <%= vars.product_short %> 1.4.1](#v1.4.1-known-issues)
* [Known Issues in <%= vars.product_short %> 1.4.0](#v1.4.0-known-issues)

## <a id="1.4.1"></a>v1.4.1

**Release Date**: June 20, 2019

### <a id="v1.4.1-features"></a>Features

New features and changes in this release:

* Support for VMware NSX-T v2.4.1.
For more information, see [VMware NSX-T Data Center 2.4.1 Release Notes](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4.1/rn/VMware-NSX-T-Data-Center-241-Release-Notes.html).
* Support for VMware NCP v2.4.1.
For more information, see [NSX Container Plugin 2.4 Release Notes](https://docs.vmware.com/en/VMware-NSX-T-Data-Center/2.4/rn/NSX-Container-Plugin-Release-Notes.html) in the VMware documentation.
* Support for provisioning an NSX-T load balancer in front of the NSX management cluster.
For more information, see [Provisioning a Load Balancer for the NSX-T v2.4 Management Cluster](./nsxt-mgmt-lb.html).
* Ability to tune NCP rate limits. See either of the following Knowledge Base articles: [Tuning NCP parameters for increased PKS scaling](https://community.pivotal.io/s/article/tuning-ncp-parameters-for-increased-pks-scaling) or [Tuning NCP parameters for increased PKS Scaling in VMware Enterprise PKS environment (68085)](https://kb.vmware.com/s/article/68085).
* Configurable Kubernetes service network CIDR range. For more information, see [Networking](./installing-nsx-t.html#networking)
in _Installing Enterprise PKS on vSphere with NSX-T_.
* Configurable node IP block. For more information, see [Defining Network Profiles](./network-profiles-define.html).
* Improved functionality for the `/v1beta1/cluster` API endpoint to allow users to list and resize clusters that have compute profiles associated with them.
For more information, see [List Clusters with Compute Profile](./compute-profiles.html#list-clusters) and [Resize a Cluster with a Compute Profile](./compute-profiles.html#resize) in _Using Compute Profile (vSphere Only)_.

### <a id="v1.4.1-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.4.1</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>June 20, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.4.2+, v2.5.x, v2.6.x</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions (Azure only)</td>
        <td>v2.4.2, v2.4.3, v2.4.13, v2.5.5 and later, v2.6.x</td>
    </tr>
    <tr>
        <td>Xenial stemcell version</td>
        <td>v250.25 <br><strong>Note:</strong> To address the Zombieload CVE, upload stemcell v250.48.<br> For more information, see the <a href="https://community.pivotal.io/s/article/mitigating-zombieload-for-pivotal-container-service-pks">Mitigating Zombieload for Pivotal Container Service (PKS)</a> KB article.</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.13.5</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.26.0</td>
    </tr>
    <tr>
        <td>NSX-T versions</td>
        <td>v2.3.1, v2.4.0.1, v2.4.1 (recommended), v2.4.2 (<a href="#nsxt-242-ki">see below</a>)</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.4.1</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.3-ce</td>
    </tr>
    <tr>
        <td>Backup and Restore SDK version</td>
        <td>v1.8.0</td>
    </tr>
    <tr>
        <td>UAA version</td>
        <td>v71.0</td>
    </tr>
</table>

<p class="note warning"><strong>Warning:</strong> NSX-T v2.4 implements a new password expiration policy.
By default, the administrator password expires after 90 days. If the password expires,
you cannot deploy new Kubernetes clusters using PKS with NSX-T. For more information, see the
following <a href="https://kb.vmware.com/s/article/70691">VMware KB article</a>
and <a href="./nsxt-deploy-24.html#nsxt-24-password">Updating the NSX Manager Password for NSX-T v2.4</a> in the
<%= vars.product_short %> documentation.</p>

<p class="note"><strong>Note:</strong> NSX-T v2.4 implements a new user interface (UI) based on the NSX Policy API.
PKS v1.4.1 does not support the NSX Policy API. Any objects created through the new UI cannot be used with PKS v1.4.1.
If you are installing PKS v1.4.1 with NSX-T v2.4.x or upgrading to PKS v1.4.1 and NSX-T 2.4.x,
you must use the <strong>Advanced Networking</strong> tab in NSX Manager to create, read, update, and delete
all networking objects required for <%= vars.product_short %>. For more information, see <a href="./nsxt-deploy-24.html#ui-nsxt-24">NSX-T v2.4 Management Interfaces</a> in the
<%= vars.product_short %> documentation.</p>

### <a id='vsphere-reqs'></a> vSphere Version Requirements

If you are installing <%= vars.product_short %> on vSphere or on vSphere with NSX-T Data Center,
see the <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a> for compatibility information.</p>

### <a id="v1.4.1-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</sup></td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
</table>

<sup>&#42;</sup> For more information about configuring Service `type:LoadBalancer` on AWS, see the [Access Workloads Using an Internal AWS Load Balancer](deploy-workloads.html#internal-lb) section of _Deploying and Exposing Basic Workloads_.

### <a id="v1.4.1-upgrade"></a>Upgrade Path

The supported upgrade paths to PKS v1.4.1 are from PKS v1.4.0 and PKS v1.3.2 and later.

<p class="note warning"><strong>WARNING:</strong> Before you upgrade to PKS 1.4.x, you must increase the size of persistent disk for the PKS VM. 
    For more information, 
    see <a href="#not-enough-diskspace">Upgrade Fails Due to Insufficient Disk Space</a> in the Known Issues.</p>

To upgrade, see [Upgrading <%= vars.product_short %>](upgrade-pks.html) and [Upgrading <%= vars.product_short %> with NSX-T](upgrade-pks-nsxt.html).

<p class="note breaking"><strong>Breaking change:</strong> Upgrading to Ops Manager v2.6 on vSphere requires that you set a public SSH key in the OVF template prior to the upgrade. For more information, see <a href="#no-password-om">Passwords Not Supported for Ops Manager VM on vSphere</a> in the Breaking Changes section.</p>

### <a id="v1.4.1-breaking-changes"></a> Breaking Changes

Ops Manager v2.6 has the following breaking change that affects <%= vars.product_short %> v1.4.1 and later:

#### <a name='no-password-om'></a> Passwords Not Supported for Ops Manager VM on vSphere

<%= vars.product_short %> v1.4.1 and later support Ops Manager v2.6. Starting in Ops Manager v2.6, you can
only SSH onto the Ops Manager VM in a vSphere deployment with a private SSH key. You cannot SSH
onto the Ops Manager VM with a password.

To avoid upgrade failure and errors when authenticating, add a public key to the **Customize Template** screen of the the OVF template for the Ops Manager VM. Then, use the private key to SSH onto the Ops Manager VM. 

<p class="note warning"><strong>Warning</strong>: You cannot upgrade to Ops Manager v2.6 successfully without adding a public key. If you do not add a key, Ops Manager shuts down automatically because it cannot find a key and may enter a reboot loop.</p>

For more information about adding a public key to the OVF template, see [Deploy Ops Manager](https://docs.pivotal.io/pivotalcf/2-6/om/vsphere/deploy.html#deploy) in
_Deploying Ops Manager on vSphere_.

For instructions to generate an SSH key pair, see one of the following:

* [Generate an SSH key pair for installing Ops Manager v2.6 on vSphere](https://community.pivotal.io/s/article/generate-an-ssh-key-pair-for-installing-ops-manager-v2-6-on-vsphere) in the Pivotal Support Knowledge Base
* [How to generate an SSH key pair for installing Ops Manager v2.6 on vSphere](https://kb.vmware.com/s/article/71143) in the VMware Knowledge Base

<p class="note"><strong>Note:</strong> For information about breaking changes in <%= vars.product_short %> v1.4,
see <a href="#v1.4.0-breaking-changes">Breaking Changes</a> in <em>v1.4.0</em>.</p>

### <a id="v1.4.1-known-issues"></a> Known Issues

<%= vars.product_short %> v1.4.1 has the following known issues:

#### <a id='azure-avail-sets'></a> Azure Availability Sets Not Supported 

For new Enterprise PKS 1.4.1 or later installations on Azure using Ops Manager v2.5.x or v2.6.x, enabling the **Availability Sets** mode at the **BOSH Director** > **Azure Config** results in the kubelet failing to start on provisioning of a Kubernetes cluster. 

When installing Enterprise PKS on Azure, choose the **Availability Zones** option. For configuration details, see [Configuring BOSH Director on Azure](https://docs.pivotal.io/pcf/om/2-5/azure/config-terraform.html#azure-config).

#### <a id='nsxt-242-ki'></a> Support for NSX-T v2.4.2 with Known Issue and Workaround

<%= vars.product_short %> v1.4.1 supports NSX-T v2.4.2. However, there is a known issue with NSX-T v2.4.2 that can affect new and upgraded
installations of <%= vars.product_short %> v1.4.1 that use a [NAT topology](./nsxt-topologies.html#topology-nat).

For NSX-T v2.4.2, the PKS Management Plane must be deployed on a Tier-1 distributed router (DR). If the PKS Management Plane is deployed on a Tier-1 service router (SR), the router needs to be converted. To convert an SR to a DR, refer to the [East-West traffic between workloads behind different T1 is impacted, when NAT is configured on T0 (71363)](https://kb.vmware.com/s/article/71363) KB article.

This issue will be addressed in a subsequent release of NSX-T such that it will not matter if the Tier-1 Router is a DR or an SR.

#### <a name="vm-sizes"></a> PKS-provisioned Kubernetes cluster creation fails in a vSphere with NSX-T environment

**Symptom**

Kubernetes cluster creation fails with Enterprise PKS installed on vSphere with NSX-T.

**Explanation**

When configuring a plan for Kubernetes clusters, for the **Worker VM Type** setting, selecting either of the following Worker VM Types results in cluster creation failure because there is not enough disk to handle swap space. 

- `medium (cpu: 2, ram: 4 GM, disk: 8 GB)`
- `medium.mem (cpu: 1, ram: 8 GM, disk: 8 GB)`

BOSH will assign the same amount of RAM to swap up to 50% of the ephemeral disk size. For more information, see [How much Swap Space is Allocated for BOSH-Deployed VMs](https://community.pivotal.io/s/article/How-much-Swap-Space-is-Allocated-for-BOSH-Deployed-VMs) in the Pivotal Support Knowledge Base. Given the swap space requirements, the `medium` and `medium.mem` Worker VM types are only allocated 4 GB usable ephemeral disk, which is not enough for successful cluster deployment of the PKS tile with NCP. 

**Workaround**

When selecting the Worker VM Type for Worker nodes, do not use the `medium` or `medium.mem` type in a vSphere with NSX-T environment. Use the default Worker VM Type `medium.disk (cpu: 2, ram: 4 GB, disk: 32 GB)` or any other Worker VM Type **except** `medium` or `medium.mem`. 

#### <a name="failed-clusters"></a> Upgrade All Service Instances Errand Fails

**Symptom**

After clicking **Apply Changes** in Ops Manager, 
the `upgrade-all-service-instances` errand fails.

**Explanation**

The `upgrade-all-service-instances` errand fails if one or more 
of your existing clusters are in a **FAILED** state.

To check cluster status, run `pks clusters`.

**Workaround**

If you experience this issue, delete and re-create the failed cluster. 
Follow the procedure in 
[Cannot Re-Create a Cluster that Failed to Deploy](troubleshoot-issues.html#cluster-recreate-fails).

#### <a name="other-issues"></a> Other Known Issues in <%= vars.product_short %> v1.4.1

In addition to the known issues listed above,
see [Known Issues in <%= vars.product_short %> 1.4.0](#v1.4.0-known-issues).

## <a id="v1.4.0"></a>v1.4.0

**Release Date**: April 25, 2019

### <a id="v1.4.0-features"></a>Features

New features and changes in this release:

* Operators can configure up to ten sets of resource types, or _plans_, in the Enterprise PKS tile. All
plans except the first can made available or unavailable to developers deploying clusters. Plan 1
must be configured and made available as a default for developers. For more information, see the _Plans_ section of the installation topic for your IaaS, such as [Installing <%= vars.product_short %> on vSphere with NSX-T](./installing-nsx-t.html#plans).

* Operators can deploy up to 5 master nodes per plan. For more information, see the _Plans_ section of the installation topic for your IaaS, such as [Installing <%= vars.product_short %> on vSphere with NSX-T](./installing-nsx-t.html#plans).

* Operators can install PKS and Pivotal Application Service (PAS) on the same instance of Ops
Manager.

* Improved workflow for managing cluster access. For more information, see [Grant Cluster Access](manage-users.html#cluster-access) in _Managing Users in <%= vars.product_short %> with UAA_.

* Operators can create webhook ClusterSink resources. A webhook ClusterSink resource batches logs into 1 second units, wraps the resulting payload in JSON, and uses the POST method to deliver the logs to the address of your log management service. For more information see, [Create a Webhook ClusterSink Resource with YAML and kubectl](create-sinks.html#webhook-cluster-sink) in _Creating Sinks_.

* Operators can set quotas for maximum memory and CPU utilization in a PKS deployment. For more information, see [Managing Resource Usage](resource-usage.html). This is a beta feature.
    <%= partial 'beta-component' %>

* Operators can enable the PodSecurityPolicy admission plugin on a per-plan basis requiring cluster users to have policy, role, and role binding permissions to deploy pod workloads. See [Pod Security Policies](./pod-security-policy.html) for more information.

* Operators can enable the SecurityContextDeny admission plugin on a per-plan basis to prohibit the use of security context configurations on pods and containers. See [Security Context Deny](./security-context-deny.html) for more information.

* Operators can enable the DenyEscalatingExec admission plugin on a per-plan basis to prohibit the use of certain commands for containers that allow host access. See [Deny Escalating Execution](./deny-escalating-execution.html) for more information.

* Operators using vSphere can use HostGroups to define Availability Zones (AZs) for clusters in BOSH. See [Using vSphere Host Groups](./vsphere-host-group.html).

* Operators using vSphere can configure compute profiles to specify which vSphere
resources are used when deploying Kubernetes clusters in a PKS deployment.
For more information, see [Using Compute Profiles (vSphere Only)](compute-profiles.html).
    <%= partial 'beta-component' %>

* Operators using vSphere with NSX-T can update a Network Profile and add to or reorder the **Pods IP Block** IDs. For more information, see the [Change the Network Profile for a Cluster](./network-profiles.html#update-profile) section of _Using Network Profiles (NSX-T Only)_.

### <a id="v1.4.0-snapshot"></a>Product Snapshot

<table class="nice">
    <th>Element</th>
    <th>Details</th>
    <tr>
        <td>Version</td>
        <td>v1.4.0</td>
    </tr>
    <tr>
        <td>Release date</td>
        <td>April 25, 2019</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions</td>
        <td>v2.4.2+, v2.5.x</td>
    </tr>
    <tr>
        <td>Compatible Ops Manager versions (Azure only)</td>
        <td>v2.4.2, v2.4.3, v2.4.13, v2.5.5 and later</td>
    </tr>
    <tr>
        <td>Xenial stemcell version</td>
        <td>v250.25</td>
    </tr>
    <tr>
        <td>Kubernetes version</td>
        <td>v1.13.5</td>
    </tr>
    <tr>
        <td>On-Demand Broker version</td>
        <td>v0.26.0</td>
    </tr>
    <tr>
        <td>NSX-T versions <strong>&#42;</strong></td>
        <td>v2.3.1, v2.4.0.1</td>
    </tr>
    <tr>
        <td>NCP version</td>
        <td>v2.4.0</td>
    </tr>
    <tr>
        <td>Docker version</td>
        <td>v18.06.3-ce</td>
    </tr>
    <tr>
        <td>Backup and Restore SDK version</td>
        <td>v1.8.0</td>
    </tr>
    <tr>
        <td>UAA version</td>
        <td>v71.0</td>
    </tr>
</table>

<p class="note"><strong>WARNING:</strong> NSX-T v2.4 implements a new <em>password expiration policy</em>. 
By default the administrator password expires after 90 days. If the password expires, 
you cannot deploy new Kubernetes clusters using PKS with NSX-T. For more information, refer to the
following VMware KB article: <a href="https://kb.vmware.com/s/article/70691">https://kb.vmware.com/s/article/70691</a>. 
See also <a href="./nsxt-deploy-24.html#nsxt-24-password">Updating the NSX Manager Password for NSX-T v2.4</a> in the 
<%= vars.product_short %> documentation.</p>

<p class="note"><strong>Note:</strong> NSX-T v2.4 implements a new user interface (UI) based on the NSX Policy API. 
PKS v1.4.0 does not support the NSX Policy API. Any objects created via the new Policy-based UI cannot be used with PKS v1.4.0.
If you are installing PKS v1.4.0 with NSX-T v2.4.x, or upgrading to PKS v1.4.0 and NSX-T 2.4.x, 
you must use the "Advanced Networking" tab in NSX Manager to create, read, update, and delete 
all networking objects required for PKS. See also <a href="./nsxt-deploy-24.html#ui-nsxt-24">NSX-T v2.4 Management Interfaces</a> in the 
<%= vars.product_short %> documentation.</p>

### <a id='vsphere-reqs'></a> vSphere Version Requirements

For <%= vars.product_short %> installations on vSphere or on vSphere with NSX-T Data Center, refer to the <a href="https://www.vmware.com/resources/compatibility/sim/interop_matrix.php#interop&356=&175=&1=">VMware Product Interoperability Matrices</a>.</p>

### <a id="v1.4.0-iaas"></a>Feature Support by IaaS

<table>
  <tr>
    <th></th>
    <th>AWS</th>
    <th>Azure</th>
    <th>GCP</th>
    <th>vSphere</th>
    <th>vSphere with NSX-T</th>
  </tr>

  <tr>
    <th>Automatic Kubernetes Cluster API load balancer</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>HTTP proxy</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Multi-AZ storage</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td>&check;</td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Per-namespace subnets</th>
    <td></td>
    <td></sup></td>
    <td></td>
    <td></td>
    <td>&check;</td>
  </tr>
  <tr>
    <th>Service <code>type:LoadBalancer</code></th>
    <td>&check;<sup>&#42;</sup></td>
    <td>&check;</sup></td>
    <td>&check;</td>
    <td></td>
    <td>&check;</td>
  </tr>
</table>

<sup>&#42;</sup> For more information about configuring Service `type:LoadBalancer` on AWS, see the [Access Workloads Using an Internal AWS Load Balancer](deploy-workloads.html#internal-lb) section of _Deploying and Exposing Basic Workloads_.

### <a id="v1.4.0-upgrade"></a>Upgrade Path

The supported upgrade paths to PKS v1.4.0 are from PKS v1.3.2 and later.

<p class="note warning"><strong>WARNING:</strong> Before you upgrade to PKS 1.4.x, you must increase the size of persistent disk for the PKS VM. 
    For more information, 
    see <a href="#not-enough-diskspace">Upgrade Fails Due to Insufficient Disk Space</a> in the Known Issues.</p>

To upgrade, see [Upgrading <%= vars.product_short %>](upgrade-pks.html) and [Upgrading <%= vars.product_short %> with NSX-T](upgrade-pks-nsxt.html).

When upgrading to NSX-T 2.4:

- Use the official VMware NSX-T Data Center 2.4 build.
- Apply the NSX-T v2.4.0.1 hot-patch. For more information, see [KB article 67499](https://kb.vmware.com/s/article/67449) in the VMware Knowledge Base.
- To obtain the NSX-T v2.4.0.1 hot-patch, open a support ticket with VMware Global Support Services (GSS) for NSX-T Engineering.

### <a id="v1.4.0-breaking-changes"></a>Breaking Changes

<%= vars.product_short %> v1.4 has the following breaking change:

#### <a name='log-sink-entry-change'></a> Log Sink Entry Format Change

Log sink entries in PKS v1.4 are tagged with the human-readable cluster name instead of the host ID of the BOSH-defined VM.

### <a id="v1.4.0-known-issues"></a>Known Issues

<%= vars.product_short %> v1.4.0 has the following known issues:

#### <a name="security-group"></a>Azure Default Security Group Is Not Automatically Assigned to Cluster VMs

**Symptom**

You experience issues when configuring a load balancer for a multi-master Kubernetes cluster or creating a service of type `LoadBalancer`.
Additionally, in the Azure portal, the **VM** > **Networking** page does not display
any inbound and outbound traffic rules for your cluster VMs.

**Explanation**

As part of configuring the <%= vars.product_tile %> tile for Azure, you enter **Default Security Group** in the **Kubernetes Cloud Provider** pane.
When you create a Kubernetes cluster, <%= vars.product_short %> automatically assigns this security group to each VM in the cluster.
However, in <%= vars.product_short %> v1.4, the automatic assignment may not occur.

As a result, your inbound and outbound traffic rules defined in the security group are not applied to the cluster VMs.

**Workaround**

If you experience this issue, manually assign the default security group to each VM NIC in your cluster.

#### <a id='first-az'></a>Cluster Creation Fails When First AZ Runs out of Resources

**Symptom**

If the first availability zone (AZ) used by a plan with multiple AZs runs out of
resources, cluster creation fails with an error like the following:

<pre class="terminal">
L Error: CPI error 'Bosh::Clouds::CloudError' with message 'No valid placement found for requested memory: 4096
</pre>

**Explanation**

BOSH creates VMs for your <%= vars.product_short %> deployment using a round-robin
algorithm, creating the first VM in the first AZ that your plan uses.
If the AZ runs out of resources, cluster creation fails because BOSH cannot create
the cluster VM.

For example, if your three AZs each have enough resources for ten VMs, and you
create two clusters with four worker VMs each, BOSH creates VMs in the
following AZs:

<table>
  <tr>
    <th></th>
    <th>AZ1</th>
    <th>AZ2</th>
    <th>AZ3</th>
  </tr>
  <tr>
    <th>Cluster 1</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
  </tr>
  <tr>
    <th>Cluster 2</th>
    <td>Worker VM 1</td>
    <td>Worker VM 2</td>
    <td>Worker VM 3</td>
  </tr>
  <tr>
    <td></td>
    <td>Worker VM 4</td>
    <td></td>
    <td></td>
</table>

In this scenario, AZ1 has twice as many VMs as AZ2 or AZ3.

#### <a id='azure-worker-comm'></a>Azure Worker Node Communication Fails after Upgrade

**Symptom**

Outbound communication from a worker node VM fails after an upgrade to <%= vars.product_short %> v1.4.0.

**Explanation**

<%= vars.product_short %> 1.4.0 uses Azure Availability Sets to improve the uptime of workloads and worker nodes in the event of Azure platform failures. Worker node
VMs are distributed evenly across Availability Sets.

Azure Standard SKU Load Balancers are recommended for the Kubernetes control plane and Kubernetes ingress and egress. This load balancer type provides an IP address for outbound communication using SNAT.

During an upgrade, when BOSH rebuilds a given worker instance in an Availability Set,
Azure can time out while re-attaching the worker node network interface to the
back-end pool of the Standard SKU Load Balancer.

For more information, see [Outbound connections in Azure](https://docs.microsoft.com/en-us/azure/load-balancer/load-balancer-outbound-connections) in the Azure documentation.

**Workaround**

You can manually re-attach the worker instance to the back-end pool of the Azure Standard SKU Load Balancer in your Azure console.

#### <a name="not-enough-diskspace"></a> Upgrade Fails Due to Insufficient Disk Space

**Symptom**

The upgrade from PKS v1.3.x to <%= vars.product_short %> v1.4.x fails with the following error in the BOSH debug log:

```
1 of 8 pre-start scripts failed. Failed Jobs:pxc-mysql
```

The pxc-mysql pre-start logs also include the following error message:

```
panic: Cannot continue, insufficient disk space to complete migration
```

**Explanation**

The upgrade to <%= vars.product_short %> v1.4.x includes a MySQL migration from MaridDB to Percona.

When `/var/vcap/store` does not have enough space for the backup of existing MySQL files
that happens during this migration, the <%= vars.product_short %> upgrade fails.

**Workaround**

Before upgrading to <%= vars.product_short %> v1.4.x, increase the persistent disk type configured for the PKS VM.

1. In Ops Manager, click the **Pivotal Container Service** tile.
1. Select the **Resource Config** pane.
1. In **Pivotal Container Service** > **Persistent Disk Type**, select a larger disk type than the currently configured disk type.
For example, if `10 GB` is currently configured, select `20 GB`.
 
If you experience this issue after attempting an upgrade, perform the steps above and start the upgrade again.


#### <a id="kubectl-azs"></a> Kubectl CLI Commands Do Not Work after Changing an Existing Plan to a Different AZ

**Symptom**

After you reconfigure the AZ of an existing plan, kubectl cli commands do not work in the plan's existing clusters.

**Explanation**

This issue occurs in IaaS environments which either limit or prevent attaching a disk across multiple AZs.

BOSH supports creating new VMs and attaching existing disks to VMs. BOSH cannot "move" VMs.

If the plan for an existing cluster is changed to a different AZ,
the cluster's new "intended" state is for the cluster to be hosted within the new AZ.
To migrate the cluster from its original state to its intended state,
BOSH will create new VMs for the cluster within the designated AZ and remove the cluster's original VMs from the original AZ.

On an IaaS where attaching VM disks across AZs is not supported, the disks attached to the newly created VMs will not have the original disks' content.

**Workaround**

If you have reconfigured the AZ of an existing cluster and afterwards could not run kubectl cli commands, contact Support for assistance.

#### <a name='uuid-length-1-4-0'></a> One Plan ID Is Longer Than Other Plan IDs

**Symptom**

One of your Plan IDs is one character longer than your other Plan IDs.

**Explanation**

Each Plan has a unique Plan ID. A Plan ID is normally a UUID consisting of 32 alphanumeric characters and 4 hyphens. 
The **Plan 4** Plan ID is instead a UUID consisting of 33 alphanumeric characters and 4 hyphens.  

You can safely configure and use **Plan 4**. 
The length of the **Plan 4** Plan ID does not affect the functionality of **Plan 4** clusters.  

If you require all Plan IDs to have identical length, do not activate or use **Plan 4**.  
